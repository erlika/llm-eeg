{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# LLM-EEG Framework - Phase 3: Feature Extraction & Classification\n\nThis notebook demonstrates the complete Phase 3 implementation for the LLM-EEG framework,\nfocused on feature extraction and classification for motor imagery EEG signals.\n\n## Overview\n\n**Phase 3 Components:**\n- **Feature Extractors**: CSP, Band Power, Time Domain\n- **Feature Pipeline**: Modular feature extraction with multiple extractors\n- **Classifiers**: LDA, SVM, EEGNet\n- **Evaluation**: Cross-validation, metrics, model comparison\n\n**Building on Phase 2:**\n- Data loading (BCICIV2aLoader)\n- Preprocessing (Bandpass, Notch, Normalization)\n- PyTorch datasets\n\n**Performance Targets:**\n- Subject-dependent accuracy: >85%\n- Subject-independent accuracy: >70%\n- Cohen's Kappa: >0.80\n\n---",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Step 1: Environment Setup",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Step 1.1: Mount Google Drive\nfrom google.colab import drive\ndrive.mount('/content/drive')",
      "execution_count": null,
      "metadata": {},
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Step 1.2: Clone the repository\n!git clone https://github.com/erlika/llm-eeg.git\n%cd llm-eeg",
      "execution_count": null,
      "metadata": {},
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Step 1.3: Install dependencies\n!pip install -q numpy scipy mne torch scikit-learn matplotlib seaborn",
      "execution_count": null,
      "metadata": {},
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Step 1.4: Add src to Python path and verify imports\nimport sys\nsys.path.insert(0, '/content/llm-eeg')\n\n# Verify Phase 3 imports\nfrom src.features import (\n    CSPExtractor, BandPowerExtractor, TimeDomainExtractor,\n    FeatureExtractorFactory, FeatureExtractionPipeline,\n    create_csp_extractor, create_motor_imagery_pipeline\n)\nfrom src.classifiers import (\n    LDAClassifier, SVMClassifier, EEGNetClassifier,\n    ClassifierFactory, create_lda_classifier, create_svm_classifier,\n    create_eegnet_classifier, list_available_classifiers\n)\n\nprint(\"✅ Phase 3 modules imported successfully!\")\nprint(f\"\\nAvailable classifiers: {list_available_classifiers()}\")",
      "execution_count": null,
      "metadata": {},
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## Step 2: Load and Preprocess Data (Phase 2 Review)",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Step 2.1: Configure data paths\nimport os\nimport numpy as np\n\n# Update this path to your Google Drive location\nDATA_DIR = '/content/drive/MyDrive/BCI_Data/dataset_2a'\n\n# Alternative paths:\n# DATA_DIR = '/content/drive/MyDrive/BCI_Competition_IV_2a'\n\nif os.path.exists(DATA_DIR):\n    files = os.listdir(DATA_DIR)\n    mat_files = [f for f in files if f.endswith('.mat')]\n    print(f\"✅ Found {len(mat_files)} MAT files in {DATA_DIR}\")\nelse:\n    print(f\"❌ Directory not found: {DATA_DIR}\")\n    print(\"Please update DATA_DIR to your dataset location\")",
      "execution_count": null,
      "metadata": {},
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Step 2.2: Load data using Phase 2 BCICIV2aLoader\nfrom scipy.io import loadmat\nfrom src.core.data_types import EEGData, EventMarker\n\nclass BCICIV2aLoader:\n    \"\"\"Data loader for BCI Competition IV-2a dataset.\"\"\"\n    \n    def __init__(self, sampling_rate=250, include_eog=False, trial_duration=4.0, trial_offset=0.0):\n        self.sampling_rate = sampling_rate\n        self.n_eeg_channels = 22\n        self.include_eog = include_eog\n        self.trial_duration = trial_duration\n        self.trial_offset = trial_offset\n        self.class_mapping = {1: 'left_hand', 2: 'right_hand', 3: 'feet', 4: 'tongue'}\n        self.eeg_channel_names = [\n            'Fz', 'FC3', 'FC1', 'FCz', 'FC2', 'FC4',\n            'C5', 'C3', 'C1', 'Cz', 'C2', 'C4', 'C6',\n            'CP3', 'CP1', 'CPz', 'CP2', 'CP4', 'P1', 'Pz', 'P2', 'POz'\n        ]\n        \n    def load(self, file_path):\n        mat_data = loadmat(file_path, struct_as_record=False, squeeze_me=True)\n        data_array = mat_data['data']\n        \n        all_signals = []\n        all_events = []\n        sample_offset = 0\n        \n        for run_idx in range(len(data_array)):\n            run = data_array[run_idx]\n            signals = run.X\n            n_samples = signals.shape[0]\n            all_signals.append(signals)\n            \n            if hasattr(run, 'y') and hasattr(run.y, '__len__') and len(run.y) > 0:\n                for start, label in zip(run.trial, run.y):\n                    event = EventMarker(\n                        sample=int(start) + sample_offset,\n                        code=768 + int(label),\n                        label=self.class_mapping.get(int(label), f'class_{label}')\n                    )\n                    all_events.append(event)\n            sample_offset += n_samples\n        \n        signals = np.vstack(all_signals).T[:self.n_eeg_channels, :]\n        \n        return EEGData(\n            signals=signals,\n            sampling_rate=self.sampling_rate,\n            channel_names=self.eeg_channel_names,\n            events=all_events\n        )\n    \n    def extract_trials(self, eeg_data, duration=None, offset=None):\n        duration = duration or self.trial_duration\n        offset = offset or self.trial_offset\n        samples_per_trial = int(duration * self.sampling_rate)\n        offset_samples = int(offset * self.sampling_rate)\n        \n        trials, labels = [], []\n        for event in eeg_data.events:\n            start = event.sample + offset_samples\n            end = start + samples_per_trial\n            if start < 0 or end > eeg_data.signals.shape[1]:\n                continue\n            trials.append(eeg_data.signals[:, start:end])\n            labels.append(event.code - 769)\n        \n        return np.array(trials), np.array(labels)\n\nprint(\"✅ BCICIV2aLoader ready\")",
      "execution_count": null,
      "metadata": {},
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Step 2.3: Load Subject A01 Training Data\nloader = BCICIV2aLoader()\n\nsubject_file = os.path.join(DATA_DIR, 'A01T.mat')\neeg_data = loader.load(subject_file)\nX, y = loader.extract_trials(eeg_data)\n\nprint(f\"\\n=== Subject A01 Data ===\")\nprint(f\"Trials shape: {X.shape}\")\nprint(f\"Labels shape: {y.shape}\")\nprint(f\"Classes: {np.unique(y)}\")\nprint(f\"Samples per class: {[np.sum(y==c) for c in range(4)]}\")",
      "execution_count": null,
      "metadata": {},
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Step 2.4: Preprocess Data\nfrom src.preprocessing import create_standard_pipeline\n\npipeline = create_standard_pipeline(\n    sampling_rate=250,\n    notch_freq=50.0,\n    low_freq=8.0,\n    high_freq=30.0,\n    normalize_method='zscore'\n)\npipeline.initialize()\n\nX_processed = pipeline.process(X)\nprint(f\"\\n=== Preprocessed Data ===\")\nprint(f\"Shape: {X_processed.shape}\")\nprint(f\"Range: [{X_processed.min():.2f}, {X_processed.max():.2f}]\")",
      "execution_count": null,
      "metadata": {},
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Step 2.5: Split Data\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X_processed, y, test_size=0.2, random_state=42, stratify=y\n)\n\nprint(f\"\\n=== Data Split ===\")\nprint(f\"Train: {X_train.shape}\")\nprint(f\"Test: {X_test.shape}\")",
      "execution_count": null,
      "metadata": {},
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## Step 3: CSP Feature Extraction\n\nCommon Spatial Pattern (CSP) is the most effective spatial filtering technique for motor imagery EEG classification.\n\n**How CSP works:**\n1. Learn spatial filters that maximize variance for one class while minimizing for another\n2. Project EEG data through these filters\n3. Compute log-variance features",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Step 3.1: Create and fit CSP extractor\nfrom src.features import CSPExtractor, create_csp_extractor\n\n# Create CSP with 6 components (3 per class for binary)\ncsp = create_csp_extractor(n_components=6, sampling_rate=250)\n\n# Fit and extract features\nX_train_csp = csp.fit_extract(X_train, y_train)\nX_test_csp = csp.extract(X_test)\n\nprint(f\"\\n=== CSP Feature Extraction ===\")\nprint(f\"Original shape: {X_train.shape}\")\nprint(f\"CSP features shape: {X_train_csp.shape}\")\nprint(f\"Feature names: {csp.get_feature_names()[:6]}\")\nprint(f\"\\nFilters shape: {csp.get_spatial_filters().shape}\")\nprint(f\"Patterns shape: {csp.get_spatial_patterns().shape}\")",
      "execution_count": null,
      "metadata": {},
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Step 3.2: Visualize CSP Spatial Patterns\nimport matplotlib.pyplot as plt\nimport numpy as np\n\npatterns = csp.get_spatial_patterns()\nn_patterns = min(6, patterns.shape[0])\n\nfig, axes = plt.subplots(2, 3, figsize=(12, 8))\naxes = axes.flatten()\n\nfor i in range(n_patterns):\n    ax = axes[i]\n    pattern = patterns[i]\n    \n    # Simple bar plot of channel weights\n    ax.bar(range(len(pattern)), pattern)\n    ax.set_title(f'CSP Pattern {i+1}')\n    ax.set_xlabel('Channel')\n    ax.set_ylabel('Weight')\n    ax.axhline(y=0, color='k', linestyle='-', linewidth=0.5)\n\nplt.tight_layout()\nplt.suptitle('CSP Spatial Patterns', y=1.02, fontsize=14)\nplt.show()",
      "execution_count": null,
      "metadata": {},
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Step 3.3: CSP Feature Visualization\nimport matplotlib.pyplot as plt\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Feature distribution by class\nax1 = axes[0]\nfor class_idx in range(4):\n    class_mask = y_train == class_idx\n    ax1.scatter(\n        X_train_csp[class_mask, 0], \n        X_train_csp[class_mask, 1],\n        label=f'Class {class_idx}',\n        alpha=0.6\n    )\nax1.set_xlabel('CSP Feature 1')\nax1.set_ylabel('CSP Feature 2')\nax1.set_title('CSP Features: First 2 Components')\nax1.legend()\nax1.grid(True, alpha=0.3)\n\n# Box plot of features per class\nax2 = axes[1]\nfeature_data = [X_train_csp[y_train == c, 0] for c in range(4)]\nax2.boxplot(feature_data, labels=['Left Hand', 'Right Hand', 'Feet', 'Tongue'])\nax2.set_xlabel('Class')\nax2.set_ylabel('CSP Feature 1')\nax2.set_title('CSP Feature 1 Distribution by Class')\nax2.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()",
      "execution_count": null,
      "metadata": {},
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## Step 4: Band Power Feature Extraction\n\nBand power features extract the spectral energy in specific frequency bands relevant to motor imagery:\n- **Mu (8-12 Hz)**: Sensorimotor rhythm\n- **Beta (12-30 Hz)**: Motor planning and execution",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Step 4.1: Band Power Extraction\nfrom src.features import BandPowerExtractor, create_band_power_extractor\n\n# Create band power extractor for motor imagery bands\nbands = {\n    'mu': (8, 12),\n    'beta_low': (12, 20),\n    'beta_high': (20, 30)\n}\n\nbp = create_band_power_extractor(\n    bands=bands,\n    sampling_rate=250,\n    average_channels=False,\n    log=True\n)\n\n# Extract features\nX_train_bp = bp.extract(X_train)\nX_test_bp = bp.extract(X_test)\n\nprint(f\"\\n=== Band Power Features ===\")\nprint(f\"Feature shape: {X_train_bp.shape}\")\nprint(f\"Features per trial: {X_train_bp.shape[1]} (22 channels x 3 bands)\")",
      "execution_count": null,
      "metadata": {},
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Step 4.2: Visualize Band Power\nimport matplotlib.pyplot as plt\n\n# Average band power across trials per class\nfig, axes = plt.subplots(2, 2, figsize=(12, 10))\nclass_names = ['Left Hand', 'Right Hand', 'Feet', 'Tongue']\n\nfor class_idx, ax in enumerate(axes.flatten()):\n    class_mask = y_train == class_idx\n    class_bp = X_train_bp[class_mask].mean(axis=0)\n    \n    # Reshape to (channels, bands)\n    n_channels = 22\n    n_bands = 3\n    bp_reshaped = class_bp.reshape(n_channels, n_bands)\n    \n    im = ax.imshow(bp_reshaped.T, aspect='auto', cmap='RdBu_r')\n    ax.set_xlabel('Channel')\n    ax.set_ylabel('Frequency Band')\n    ax.set_yticks([0, 1, 2])\n    ax.set_yticklabels(['Mu', 'Beta Low', 'Beta High'])\n    ax.set_title(f'{class_names[class_idx]}')\n    plt.colorbar(im, ax=ax)\n\nplt.suptitle('Average Band Power by Class', y=1.02, fontsize=14)\nplt.tight_layout()\nplt.show()",
      "execution_count": null,
      "metadata": {},
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## Step 5: Feature Pipeline\n\nCombine multiple feature extractors into a single pipeline for comprehensive feature representation.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Step 5.1: Create Feature Pipeline\nfrom src.features import FeatureExtractionPipeline, create_motor_imagery_pipeline\n\n# Create motor imagery optimized pipeline\npipeline = create_motor_imagery_pipeline(\n    n_csp_components=6,\n    sampling_rate=250\n)\n\n# Fit and extract\nX_train_features = pipeline.fit_extract(X_train, y_train)\nX_test_features = pipeline.extract(X_test)\n\nprint(f\"\\n=== Feature Pipeline ===\")\nprint(f\"Combined features shape: {X_train_features.shape}\")\nprint(pipeline.summary())",
      "execution_count": null,
      "metadata": {},
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## Step 6: Classification with LDA\n\nLinear Discriminant Analysis (LDA) is a classic and effective classifier for CSP features.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Step 6.1: Train LDA Classifier\nfrom src.classifiers import create_lda_classifier\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n\n# Create and train LDA on CSP features\nlda = create_lda_classifier(n_classes=4)\nlda.fit(X_train_csp, y_train)\n\n# Predict\ny_pred_lda = lda.predict(X_test_csp)\ny_prob_lda = lda.predict_proba(X_test_csp)\n\n# Evaluate\nacc_lda = accuracy_score(y_test, y_pred_lda)\n\nprint(f\"\\n=== CSP + LDA Results ===\")\nprint(f\"Accuracy: {acc_lda:.4f}\")\nprint(f\"\\nClassification Report:\")\nprint(classification_report(y_test, y_pred_lda, target_names=['Left', 'Right', 'Feet', 'Tongue']))",
      "execution_count": null,
      "metadata": {},
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Step 6.2: Confusion Matrix\nimport seaborn as sns\n\ncm = confusion_matrix(y_test, y_pred_lda)\n\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n            xticklabels=['Left', 'Right', 'Feet', 'Tongue'],\n            yticklabels=['Left', 'Right', 'Feet', 'Tongue'])\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.title(f'CSP + LDA Confusion Matrix (Accuracy: {acc_lda:.2%})')\nplt.show()",
      "execution_count": null,
      "metadata": {},
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## Step 7: Classification with SVM\n\nSupport Vector Machine with RBF kernel often achieves higher accuracy than LDA for complex decision boundaries.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Step 7.1: Train SVM Classifier\nfrom src.classifiers import create_svm_classifier\n\n# Create and train SVM with RBF kernel\nsvm = create_svm_classifier(kernel='rbf', C=1.0, gamma='scale', n_classes=4)\nsvm.fit(X_train_csp, y_train)\n\n# Predict\ny_pred_svm = svm.predict(X_test_csp)\nacc_svm = accuracy_score(y_test, y_pred_svm)\n\nprint(f\"\\n=== CSP + SVM Results ===\")\nprint(f\"Accuracy: {acc_svm:.4f}\")\nprint(f\"\\nClassification Report:\")\nprint(classification_report(y_test, y_pred_svm, target_names=['Left', 'Right', 'Feet', 'Tongue']))\nprint(f\"\\nSupport Vectors per class: {svm.n_support_}\")",
      "execution_count": null,
      "metadata": {},
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## Step 8: EEGNet Deep Learning Classifier\n\nEEGNet is a compact CNN designed specifically for EEG classification. It can learn directly from raw/preprocessed EEG without manual feature extraction.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Step 8.1: Create and Train EEGNet\nimport torch\nfrom src.classifiers import create_eegnet_classifier\n\n# Check device\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f\"Using device: {device}\")\n\n# Create EEGNet\neegnet = create_eegnet_classifier(\n    n_classes=4,\n    n_channels=22,\n    n_samples=1000,  # 4 seconds at 250 Hz\n    F1=8,\n    D=2,\n    dropout_rate=0.5,\n    learning_rate=0.001,\n    device=device\n)\n\nprint(f\"\\n=== EEGNet Model ===\")\nprint(f\"Parameters: {eegnet.count_parameters()}\")",
      "execution_count": null,
      "metadata": {},
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Step 8.2: Train EEGNet\n# Split training data for validation\nX_train_dl, X_val_dl, y_train_dl, y_val_dl = train_test_split(\n    X_train, y_train, test_size=0.15, random_state=42, stratify=y_train\n)\n\n# Train\nprint(\"\\n=== Training EEGNet ===\")\neegnet.fit(\n    X_train_dl.astype(np.float32), \n    y_train_dl,\n    validation_data=(X_val_dl.astype(np.float32), y_val_dl),\n    epochs=50,\n    batch_size=32,\n    verbose=1\n)",
      "execution_count": null,
      "metadata": {},
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Step 8.3: Evaluate EEGNet\ny_pred_eegnet = eegnet.predict(X_test.astype(np.float32))\nacc_eegnet = accuracy_score(y_test, y_pred_eegnet)\n\nprint(f\"\\n=== EEGNet Results ===\")\nprint(f\"Accuracy: {acc_eegnet:.4f}\")\nprint(f\"\\nClassification Report:\")\nprint(classification_report(y_test, y_pred_eegnet, target_names=['Left', 'Right', 'Feet', 'Tongue']))",
      "execution_count": null,
      "metadata": {},
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Step 8.4: Plot Training History\nhistory = eegnet.get_training_history()\n\nif history:\n    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n    \n    # Loss\n    ax1 = axes[0]\n    ax1.plot(history['train_loss'], label='Train Loss')\n    ax1.plot(history['val_loss'], label='Val Loss')\n    ax1.set_xlabel('Epoch')\n    ax1.set_ylabel('Loss')\n    ax1.set_title('EEGNet Training Loss')\n    ax1.legend()\n    ax1.grid(True, alpha=0.3)\n    \n    # Accuracy\n    ax2 = axes[1]\n    ax2.plot(history['train_accuracy'], label='Train Acc')\n    ax2.plot(history['val_accuracy'], label='Val Acc')\n    ax2.set_xlabel('Epoch')\n    ax2.set_ylabel('Accuracy')\n    ax2.set_title('EEGNet Training Accuracy')\n    ax2.legend()\n    ax2.grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.show()",
      "execution_count": null,
      "metadata": {},
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## Step 9: Model Comparison",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Step 9.1: Compare All Models\nfrom sklearn.metrics import cohen_kappa_score\n\nresults = {\n    'Model': ['CSP + LDA', 'CSP + SVM', 'EEGNet'],\n    'Accuracy': [acc_lda, acc_svm, acc_eegnet],\n    'Kappa': [\n        cohen_kappa_score(y_test, y_pred_lda),\n        cohen_kappa_score(y_test, y_pred_svm),\n        cohen_kappa_score(y_test, y_pred_eegnet)\n    ]\n}\n\nimport pandas as pd\nresults_df = pd.DataFrame(results)\nresults_df = results_df.sort_values('Accuracy', ascending=False)\n\nprint(\"\\n=== Model Comparison ===\")\nprint(results_df.to_string(index=False))\n\n# Visualization\nfig, ax = plt.subplots(figsize=(10, 5))\nx = np.arange(len(results['Model']))\nwidth = 0.35\n\nbars1 = ax.bar(x - width/2, results['Accuracy'], width, label='Accuracy', color='steelblue')\nbars2 = ax.bar(x + width/2, results['Kappa'], width, label='Kappa', color='darkorange')\n\nax.set_ylabel('Score')\nax.set_title('Model Comparison: Accuracy & Kappa')\nax.set_xticks(x)\nax.set_xticklabels(results['Model'])\nax.legend()\nax.axhline(y=0.85, color='green', linestyle='--', alpha=0.5, label='Target (85%)')\nax.set_ylim([0, 1])\nax.grid(True, alpha=0.3, axis='y')\n\nplt.tight_layout()\nplt.show()",
      "execution_count": null,
      "metadata": {},
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## Step 10: Cross-Subject Evaluation (LOSO)",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Step 10.1: Leave-One-Subject-Out Cross-Validation\ndef run_loso_cv(data_dir, subjects, use_csp_lda=True):\n    \"\"\"\n    Run Leave-One-Subject-Out cross-validation.\n    \n    Args:\n        data_dir: Path to dataset\n        subjects: List of subject IDs\n        use_csp_lda: If True, use CSP+LDA; else use EEGNet\n        \n    Returns:\n        Dict with results per subject and aggregate metrics\n    \"\"\"\n    loader = BCICIV2aLoader()\n    preproc = create_standard_pipeline(sampling_rate=250, notch_freq=50.0, low_freq=8.0, high_freq=30.0)\n    preproc.initialize()\n    \n    results = []\n    \n    for test_subject in subjects:\n        print(f\"\\nTest Subject: {test_subject}\")\n        \n        # Collect train and test data\n        X_train_all, y_train_all = [], []\n        X_test_sub, y_test_sub = None, None\n        \n        for subject in subjects:\n            file_path = os.path.join(data_dir, f\"{subject}T.mat\")\n            if not os.path.exists(file_path):\n                continue\n            \n            eeg_data = loader.load(file_path)\n            X, y = loader.extract_trials(eeg_data)\n            X = preproc.process(X)\n            \n            if subject == test_subject:\n                X_test_sub, y_test_sub = X, y\n            else:\n                X_train_all.append(X)\n                y_train_all.append(y)\n        \n        X_train_all = np.concatenate(X_train_all, axis=0)\n        y_train_all = np.concatenate(y_train_all, axis=0)\n        \n        # Train and evaluate\n        if use_csp_lda:\n            csp = create_csp_extractor(n_components=6, sampling_rate=250)\n            X_train_feat = csp.fit_extract(X_train_all, y_train_all)\n            X_test_feat = csp.extract(X_test_sub)\n            \n            clf = create_lda_classifier(n_classes=4)\n            clf.fit(X_train_feat, y_train_all)\n            y_pred = clf.predict(X_test_feat)\n        else:\n            clf = create_eegnet_classifier(\n                n_classes=4, n_channels=22, n_samples=1000, device='cpu'\n            )\n            clf.fit(X_train_all.astype(np.float32), y_train_all, epochs=30, verbose=0)\n            y_pred = clf.predict(X_test_sub.astype(np.float32))\n        \n        acc = accuracy_score(y_test_sub, y_pred)\n        kappa = cohen_kappa_score(y_test_sub, y_pred)\n        \n        results.append({\n            'subject': test_subject,\n            'accuracy': acc,\n            'kappa': kappa\n        })\n        print(f\"  Accuracy: {acc:.4f}, Kappa: {kappa:.4f}\")\n    \n    # Aggregate\n    accs = [r['accuracy'] for r in results]\n    kappas = [r['kappa'] for r in results]\n    \n    return {\n        'results': results,\n        'mean_accuracy': np.mean(accs),\n        'std_accuracy': np.std(accs),\n        'mean_kappa': np.mean(kappas),\n        'std_kappa': np.std(kappas)\n    }\n\nprint(\"✅ LOSO CV function defined\")",
      "execution_count": null,
      "metadata": {},
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Step 10.2: Run LOSO Cross-Validation\n# Uncomment to run (takes several minutes)\n# subjects = ['A01', 'A02', 'A03', 'A04', 'A05', 'A06', 'A07', 'A08', 'A09']\n# loso_results = run_loso_cv(DATA_DIR, subjects, use_csp_lda=True)\n# print(f\"\\n=== LOSO Results (CSP + LDA) ===\")\n# print(f\"Mean Accuracy: {loso_results['mean_accuracy']:.4f} ± {loso_results['std_accuracy']:.4f}\")\n# print(f\"Mean Kappa: {loso_results['mean_kappa']:.4f} ± {loso_results['std_kappa']:.4f}\")",
      "execution_count": null,
      "metadata": {},
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## Step 11: Save and Export Results",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Step 11.1: Export Results to Google Drive\nimport json\nfrom datetime import datetime\n\ndef export_results(results_dict, output_dir):\n    \"\"\"Export experiment results to JSON for sharing.\"\"\"\n    os.makedirs(output_dir, exist_ok=True)\n    \n    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n    filename = f\"phase3_results_{timestamp}.json\"\n    filepath = os.path.join(output_dir, filename)\n    \n    with open(filepath, 'w') as f:\n        json.dump(results_dict, f, indent=2)\n    \n    print(f\"Results saved to: {filepath}\")\n    return filepath\n\n# Export\noutput_dir = '/content/drive/MyDrive/llm-eeg/experiments'\nresults_to_export = {\n    'experiment': 'Phase 3 - Feature Extraction & Classification',\n    'date': datetime.now().isoformat(),\n    'subject': 'A01',\n    'models': {\n        'csp_lda': {'accuracy': float(acc_lda), 'kappa': float(cohen_kappa_score(y_test, y_pred_lda))},\n        'csp_svm': {'accuracy': float(acc_svm), 'kappa': float(cohen_kappa_score(y_test, y_pred_svm))},\n        'eegnet': {'accuracy': float(acc_eegnet), 'kappa': float(cohen_kappa_score(y_test, y_pred_eegnet))}\n    },\n    'best_model': max(['csp_lda', 'csp_svm', 'eegnet'], \n                      key=lambda m: results_to_export['models'][m]['accuracy'] if 'models' in dir() else 0)\n}\n\n# Uncomment to save\n# export_results(results_to_export, output_dir)",
      "execution_count": null,
      "metadata": {},
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Step 11.2: Save Models\nimport tempfile\n\ndef save_models(models_dict, output_dir):\n    \"\"\"Save trained models.\"\"\"\n    os.makedirs(output_dir, exist_ok=True)\n    \n    for name, model in models_dict.items():\n        if hasattr(model, 'save'):\n            filepath = os.path.join(output_dir, f\"{name}_model.pkl\")\n            model.save(filepath)\n            print(f\"Saved {name} to {filepath}\")\n\n# Save CSP extractor and classifiers\n# models_to_save = {'csp': csp, 'lda': lda, 'svm': svm}\n# save_models(models_to_save, '/content/drive/MyDrive/llm-eeg/models')",
      "execution_count": null,
      "metadata": {},
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## Step 12: Quick Reference & Summary\n\n### Results Summary\n\nPrint a comprehensive summary to share with AI assistant for analysis.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Step 12.1: Generate Summary for AI Assistant\ndef generate_summary():\n    \"\"\"Generate shareable summary.\"\"\"\n    summary = f\"\"\"\n# Phase 3 Results Summary\n\n## Dataset\n- Subject: A01\n- Trials: {X.shape[0]} ({X_train.shape[0]} train, {X_test.shape[0]} test)\n- Channels: {X.shape[1]}\n- Samples: {X.shape[2]} (4s @ 250Hz)\n- Classes: 4 (Left Hand, Right Hand, Feet, Tongue)\n\n## Feature Extraction\n- CSP: {X_train_csp.shape[1]} features ({csp._n_components} components)\n- Band Power: 3 bands (Mu, Beta-Low, Beta-High)\n\n## Classification Results\n| Model | Accuracy | Kappa |\n|-------|----------|-------|\n| CSP + LDA | {acc_lda:.4f} | {cohen_kappa_score(y_test, y_pred_lda):.4f} |\n| CSP + SVM | {acc_svm:.4f} | {cohen_kappa_score(y_test, y_pred_svm):.4f} |\n| EEGNet | {acc_eegnet:.4f} | {cohen_kappa_score(y_test, y_pred_eegnet):.4f} |\n\n## Observations\n- Best performing model: {'CSP+LDA' if acc_lda > max(acc_svm, acc_eegnet) else 'CSP+SVM' if acc_svm > acc_eegnet else 'EEGNet'}\n- Target (85%): {'Achieved ✅' if max(acc_lda, acc_svm, acc_eegnet) >= 0.85 else 'Not yet achieved ❌'}\n\"\"\"\n    return summary\n\nprint(generate_summary())",
      "execution_count": null,
      "metadata": {},
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## Phase 3 Complete!\n\n### Summary\n\nYou have successfully completed Phase 3: Feature Extraction & Classification:\n\n1. **CSP Feature Extraction**: 6 components, spatial patterns visualization\n2. **Band Power Features**: Mu, Beta frequency bands\n3. **Feature Pipeline**: Modular multi-extractor pipeline\n4. **LDA Classification**: Linear discriminant analysis on CSP features\n5. **SVM Classification**: RBF kernel SVM for non-linear boundaries\n6. **EEGNet**: End-to-end deep learning classifier\n7. **Model Comparison**: Accuracy and Kappa metrics\n8. **Cross-Validation**: LOSO setup for subject-independent evaluation\n9. **Results Export**: JSON export for AI assistant sharing\n\n### Next Steps: Phase 4 - Agent System\n\n- Adaptive Preprocessing Agent (APA)\n- Decision Validation Agent (DVA)\n- Q-learning policy optimization\n- Cross-trial learning\n\n---\n\n### Quick Reference\n\n```python\n# CSP Feature Extraction\ncsp = create_csp_extractor(n_components=6, sampling_rate=250)\nX_csp = csp.fit_extract(X_train, y_train)\nX_test_csp = csp.extract(X_test)\n\n# LDA Classification\nlda = create_lda_classifier(n_classes=4)\nlda.fit(X_csp, y_train)\npredictions = lda.predict(X_test_csp)\n\n# EEGNet\neegnet = create_eegnet_classifier(n_classes=4, n_channels=22, n_samples=1000)\neegnet.fit(X_train, y_train, epochs=50)\npredictions = eegnet.predict(X_test)\n```",
      "metadata": {}
    }
  ]
}
