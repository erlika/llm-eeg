{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# LLM-EEG Framework - Phase 3: Feature Extraction & Classification\n\nThis notebook demonstrates the complete Phase 3 implementation for the LLM-EEG framework,\nfocused on feature extraction and classification for motor imagery EEG signals.\n\n## Overview\n\n**Phase 3 Components:**\n- **Feature Extractors**: CSP, Band Power, Time Domain\n- **Feature Pipeline**: Modular feature extraction with multiple extractors\n- **Classifiers**: LDA, SVM, EEGNet\n- **Evaluation**: Cross-validation, metrics, model comparison\n\n**Building on Phase 2:**\n- Data loading (BCICIV2aLoader)\n- Preprocessing (Bandpass, Notch, Normalization)\n- PyTorch datasets\n\n**Performance Targets:**\n- Subject-dependent accuracy: >85%\n- Subject-independent accuracy: >70%\n- Cohen's Kappa: >0.80\n\n---",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Step 1: Environment Setup",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Step 1.1: Mount Google Drive\nfrom google.colab import drive\ndrive.mount('/content/drive')",
      "execution_count": null,
      "metadata": {},
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Step 1.2: Clone the repository\n!git clone https://github.com/erlika/llm-eeg.git\n%cd llm-eeg",
      "execution_count": null,
      "metadata": {},
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Step 1.3: Install dependencies\n!pip install -q numpy scipy mne torch scikit-learn matplotlib seaborn",
      "execution_count": null,
      "metadata": {},
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Step 1.4: Add src to Python path and verify imports\nimport sys\nsys.path.insert(0, '/content/llm-eeg')\n\n# Verify Phase 3 imports\nfrom src.features import (\n    CSPExtractor, BandPowerExtractor, TimeDomainExtractor,\n    FeatureExtractorFactory, FeatureExtractionPipeline,\n    create_csp_extractor, create_motor_imagery_pipeline\n)\nfrom src.classifiers import (\n    LDAClassifier, SVMClassifier, EEGNetClassifier,\n    ClassifierFactory, create_lda_classifier, create_svm_classifier,\n    create_eegnet_classifier, list_available_classifiers\n)\n\nprint(\"\u2705 Phase 3 modules imported successfully!\")\nprint(f\"\\nAvailable classifiers: {list_available_classifiers()}\")",
      "execution_count": null,
      "metadata": {},
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## Step 2: Load and Preprocess Data (Phase 2 Review)",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Step 2.1: Configure data paths\nimport os\nimport numpy as np\n\n# Update this path to your Google Drive location\nDATA_DIR = '/content/drive/MyDrive/BCI_Data/dataset_2a'\n\n# Alternative paths:\n# DATA_DIR = '/content/drive/MyDrive/BCI_Competition_IV_2a'\n\nif os.path.exists(DATA_DIR):\n    files = os.listdir(DATA_DIR)\n    mat_files = [f for f in files if f.endswith('.mat')]\n    print(f\"\u2705 Found {len(mat_files)} MAT files in {DATA_DIR}\")\nelse:\n    print(f\"\u274c Directory not found: {DATA_DIR}\")\n    print(\"Please update DATA_DIR to your dataset location\")",
      "execution_count": null,
      "metadata": {},
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Step 2.2: Load data using Phase 2 BCICIV2aLoader\nfrom scipy.io import loadmat\nfrom src.core.data_types import EEGData, EventMarker\n\nclass BCICIV2aLoader:\n    \"\"\"Data loader for BCI Competition IV-2a dataset.\"\"\"\n    \n    def __init__(self, sampling_rate=250, include_eog=False, trial_duration=4.0, trial_offset=0.0):\n        self.sampling_rate = sampling_rate\n        self.n_eeg_channels = 22\n        self.include_eog = include_eog\n        self.trial_duration = trial_duration\n        self.trial_offset = trial_offset\n        self.class_mapping = {1: 'left_hand', 2: 'right_hand', 3: 'feet', 4: 'tongue'}\n        self.eeg_channel_names = [\n            'Fz', 'FC3', 'FC1', 'FCz', 'FC2', 'FC4',\n            'C5', 'C3', 'C1', 'Cz', 'C2', 'C4', 'C6',\n            'CP3', 'CP1', 'CPz', 'CP2', 'CP4', 'P1', 'Pz', 'P2', 'POz'\n        ]\n        \n    def load(self, file_path):\n        mat_data = loadmat(file_path, struct_as_record=False, squeeze_me=True)\n        data_array = mat_data['data']\n        \n        all_signals = []\n        all_events = []\n        sample_offset = 0\n        \n        for run_idx in range(len(data_array)):\n            run = data_array[run_idx]\n            signals = run.X\n            n_samples = signals.shape[0]\n            all_signals.append(signals)\n            \n            if hasattr(run, 'y') and hasattr(run.y, '__len__') and len(run.y) > 0:\n                for start, label in zip(run.trial, run.y):\n                    event = EventMarker(\n                        sample=int(start) + sample_offset,\n                        code=768 + int(label),\n                        label=self.class_mapping.get(int(label), f'class_{label}')\n                    )\n                    all_events.append(event)\n            sample_offset += n_samples\n        \n        signals = np.vstack(all_signals).T[:self.n_eeg_channels, :]\n        \n        return EEGData(\n            signals=signals,\n            sampling_rate=self.sampling_rate,\n            channel_names=self.eeg_channel_names,\n            events=all_events\n        )\n    \n    def extract_trials(self, eeg_data, duration=None, offset=None):\n        duration = duration or self.trial_duration\n        offset = offset or self.trial_offset\n        samples_per_trial = int(duration * self.sampling_rate)\n        offset_samples = int(offset * self.sampling_rate)\n        \n        trials, labels = [], []\n        for event in eeg_data.events:\n            start = event.sample + offset_samples\n            end = start + samples_per_trial\n            if start < 0 or end > eeg_data.signals.shape[1]:\n                continue\n            trials.append(eeg_data.signals[:, start:end])\n            labels.append(event.code - 769)\n        \n        return np.array(trials), np.array(labels)\n\nprint(\"\u2705 BCICIV2aLoader ready\")",
      "execution_count": null,
      "metadata": {},
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Step 2.3: Load Subject A01 Training Data\nloader = BCICIV2aLoader()\n\nsubject_file = os.path.join(DATA_DIR, 'A01T.mat')\neeg_data = loader.load(subject_file)\nX, y = loader.extract_trials(eeg_data)\n\nprint(f\"\\n=== Subject A01 Data ===\")\nprint(f\"Trials shape: {X.shape}\")\nprint(f\"Labels shape: {y.shape}\")\nprint(f\"Classes: {np.unique(y)}\")\nprint(f\"Samples per class: {[np.sum(y==c) for c in range(4)]}\")",
      "execution_count": null,
      "metadata": {},
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Step 2.4: Preprocess Data\nfrom src.preprocessing import create_standard_pipeline\n\npipeline = create_standard_pipeline(\n    sampling_rate=250,\n    notch_freq=50.0,\n    low_freq=8.0,\n    high_freq=30.0,\n    normalize_method='zscore'\n)\npipeline.initialize()\n\nX_processed = pipeline.process(X)\nprint(f\"\\n=== Preprocessed Data ===\")\nprint(f\"Shape: {X_processed.shape}\")\nprint(f\"Range: [{X_processed.min():.2f}, {X_processed.max():.2f}]\")",
      "execution_count": null,
      "metadata": {},
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Step 2.5: Split Data\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X_processed, y, test_size=0.2, random_state=42, stratify=y\n)\n\nprint(f\"\\n=== Data Split ===\")\nprint(f\"Train: {X_train.shape}\")\nprint(f\"Test: {X_test.shape}\")",
      "execution_count": null,
      "metadata": {},
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## Step 3: CSP Feature Extraction\n\nCommon Spatial Pattern (CSP) is the most effective spatial filtering technique for motor imagery EEG classification.\n\n**How CSP works:**\n1. Learn spatial filters that maximize variance for one class while minimizing for another\n2. Project EEG data through these filters\n3. Compute log-variance features",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Step 3.1: Create and fit CSP extractor\nfrom src.features import CSPExtractor, create_csp_extractor\n\n# Create CSP with 6 components (3 per class for binary)\ncsp = create_csp_extractor(n_components=6, sampling_rate=250)\n\n# Fit and extract features\nX_train_csp = csp.fit_extract(X_train, y_train)\nX_test_csp = csp.extract(X_test)\n\nprint(f\"\\n=== CSP Feature Extraction ===\")\nprint(f\"Original shape: {X_train.shape}\")\nprint(f\"CSP features shape: {X_train_csp.shape}\")\nprint(f\"Feature names: {csp.get_feature_names()[:6]}\")\nprint(f\"\\nFilters shape: {csp.get_spatial_filters().shape}\")\nprint(f\"Patterns shape: {csp.get_spatial_patterns().shape}\")",
      "execution_count": null,
      "metadata": {},
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Step 3.2: Visualize CSP Spatial Patterns\nimport matplotlib.pyplot as plt\nimport numpy as np\n\npatterns = csp.get_spatial_patterns()\nn_patterns = min(6, patterns.shape[0])\n\nfig, axes = plt.subplots(2, 3, figsize=(12, 8))\naxes = axes.flatten()\n\nfor i in range(n_patterns):\n    ax = axes[i]\n    pattern = patterns[i]\n    \n    # Simple bar plot of channel weights\n    ax.bar(range(len(pattern)), pattern)\n    ax.set_title(f'CSP Pattern {i+1}')\n    ax.set_xlabel('Channel')\n    ax.set_ylabel('Weight')\n    ax.axhline(y=0, color='k', linestyle='-', linewidth=0.5)\n\nplt.tight_layout()\nplt.suptitle('CSP Spatial Patterns', y=1.02, fontsize=14)\nplt.show()",
      "execution_count": null,
      "metadata": {},
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Step 3.3: CSP Feature Visualization\nimport matplotlib.pyplot as plt\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Feature distribution by class\nax1 = axes[0]\nfor class_idx in range(4):\n    class_mask = y_train == class_idx\n    ax1.scatter(\n        X_train_csp[class_mask, 0], \n        X_train_csp[class_mask, 1],\n        label=f'Class {class_idx}',\n        alpha=0.6\n    )\nax1.set_xlabel('CSP Feature 1')\nax1.set_ylabel('CSP Feature 2')\nax1.set_title('CSP Features: First 2 Components')\nax1.legend()\nax1.grid(True, alpha=0.3)\n\n# Box plot of features per class\nax2 = axes[1]\nfeature_data = [X_train_csp[y_train == c, 0] for c in range(4)]\nax2.boxplot(feature_data, labels=['Left Hand', 'Right Hand', 'Feet', 'Tongue'])\nax2.set_xlabel('Class')\nax2.set_ylabel('CSP Feature 1')\nax2.set_title('CSP Feature 1 Distribution by Class')\nax2.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()",
      "execution_count": null,
      "metadata": {},
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## Step 4: Band Power Feature Extraction\n\nBand power features extract the spectral energy in specific frequency bands relevant to motor imagery:\n- **Mu (8-12 Hz)**: Sensorimotor rhythm\n- **Beta (12-30 Hz)**: Motor planning and execution",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Step 4.1: Band Power Extraction\nfrom src.features import BandPowerExtractor, create_band_power_extractor\n\n# Create band power extractor for motor imagery bands\nbands = {\n    'mu': (8, 12),\n    'beta_low': (12, 20),\n    'beta_high': (20, 30)\n}\n\nbp = create_band_power_extractor(\n    bands=bands,\n    sampling_rate=250,\n    average_channels=False,\n    log=True\n)\n\n# Extract features\nX_train_bp = bp.extract(X_train)\nX_test_bp = bp.extract(X_test)\n\nprint(f\"\\n=== Band Power Features ===\")\nprint(f\"Feature shape: {X_train_bp.shape}\")\nprint(f\"Features per trial: {X_train_bp.shape[1]} (22 channels x 3 bands)\")",
      "execution_count": null,
      "metadata": {},
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Step 4.2: Visualize Band Power\nimport matplotlib.pyplot as plt\n\n# Average band power across trials per class\nfig, axes = plt.subplots(2, 2, figsize=(12, 10))\nclass_names = ['Left Hand', 'Right Hand', 'Feet', 'Tongue']\n\nfor class_idx, ax in enumerate(axes.flatten()):\n    class_mask = y_train == class_idx\n    class_bp = X_train_bp[class_mask].mean(axis=0)\n    \n    # Reshape to (channels, bands)\n    n_channels = 22\n    n_bands = 3\n    bp_reshaped = class_bp.reshape(n_channels, n_bands)\n    \n    im = ax.imshow(bp_reshaped.T, aspect='auto', cmap='RdBu_r')\n    ax.set_xlabel('Channel')\n    ax.set_ylabel('Frequency Band')\n    ax.set_yticks([0, 1, 2])\n    ax.set_yticklabels(['Mu', 'Beta Low', 'Beta High'])\n    ax.set_title(f'{class_names[class_idx]}')\n    plt.colorbar(im, ax=ax)\n\nplt.suptitle('Average Band Power by Class', y=1.02, fontsize=14)\nplt.tight_layout()\nplt.show()",
      "execution_count": null,
      "metadata": {},
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## Step 4.3: Time Domain Feature Extraction\n\nTime domain features capture statistical and temporal properties of EEG signals:\n- **Statistical**: Mean, variance, skewness, kurtosis, RMS\n- **Hjorth Parameters**: Activity, mobility, complexity",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Step 4.3: Time Domain Features\nfrom src.features import TimeDomainExtractor, create_time_domain_extractor\n\n# Create time domain extractor with Hjorth parameters\ntd = create_time_domain_extractor(\n    features=['mean', 'variance', 'rms', 'hjorth_activity', 'hjorth_mobility', 'hjorth_complexity'],\n    sampling_rate=250\n)\n\n# Extract features\nX_train_td = td.extract(X_train)\nX_test_td = td.extract(X_test)\n\nprint(f\"\\n=== Time Domain Features ===\")\nprint(f\"Feature shape: {X_train_td.shape}\")\nprint(f\"Features: 22 channels \u00d7 6 features = 132\")",
      "execution_count": null,
      "metadata": {},
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## Step 5: Feature Pipeline\n\nCombine multiple feature extractors into a single pipeline for comprehensive feature representation.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Step 5.1: Create Feature Pipeline\nfrom src.features import FeatureExtractionPipeline, create_motor_imagery_pipeline\n\n# Create motor imagery optimized pipeline\npipeline = create_motor_imagery_pipeline(\n    n_csp_components=6,\n    sampling_rate=250\n)\n\n# Fit and extract\nX_train_features = pipeline.fit_extract(X_train, y_train)\nX_test_features = pipeline.extract(X_test)\n\nprint(f\"\\n=== Feature Pipeline ===\")\nprint(f\"Combined features shape: {X_train_features.shape}\")\nprint(pipeline.summary())",
      "execution_count": null,
      "metadata": {},
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## Step 5.2: Feature Pipeline - Modular Design & Usage Scenarios\n\nThe Feature Pipeline supports multiple modes for combining extractors:\n\n| Scenario | Description | Example |\n|----------|-------------|---------|\n| Single Extractor | Only CSP | `CSPExtractor(n_components=6)` |\n| Multiple Extractors | CSP + Band Power | Concatenate features |\n| Sequential | Filter \u2192 CSP | Process in sequence |\n| Parallel | CSP \u2225 Band Power | Extract simultaneously, then combine |\n| Conditional | Different extractor based on SNR | For APA Agent (Phase 4) |\n\n### Pipeline Modes\n- `'concatenate'`: Combine features side by side (default)\n- `'sequential'`: Output of one extractor feeds into next\n- `'parallel'`: Extract in parallel, then merge",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Step 5.2: Feature Pipeline Usage Scenarios\nfrom src.features import FeatureExtractionPipeline, FeatureExtractorFactory\n\n# ============================================\n# Scenario 1: Manual Pipeline Construction\n# ============================================\npipeline_manual = FeatureExtractionPipeline(mode='concatenate')\npipeline_manual.add_extractor(\n    CSPExtractor(n_components=6, sampling_rate=250), \n    name='csp'\n)\npipeline_manual.add_extractor(\n    BandPowerExtractor(\n        bands={'mu': (8, 12), 'beta': (12, 30)},\n        sampling_rate=250\n    ), \n    name='band_power'\n)\n\n# Fit and extract\nX_train_combined = pipeline_manual.fit_extract(X_train, y_train)\nX_test_combined = pipeline_manual.extract(X_test)\n\nprint(\"=== Scenario 1: Manual Pipeline ===\")\nprint(f\"CSP features: 6\")\nprint(f\"Band Power features: 22 channels \u00d7 2 bands = 44\")\nprint(f\"Combined shape: {X_train_combined.shape}\")\n\n# ============================================\n# Scenario 2: Factory-based Creation\n# ============================================\ncsp_from_factory = FeatureExtractorFactory.create('csp', n_components=4)\nbp_from_factory = FeatureExtractorFactory.create('band_power', \n    bands={'mu': (8, 12)}, \n    sampling_rate=250\n)\n\nprint(\"\\n=== Scenario 2: Factory Creation ===\")\nprint(f\"Available extractors: {FeatureExtractorFactory.list_available()}\")\n\n# ============================================\n# Scenario 3: Config-driven Pipeline (for Agents)\n# ============================================\npipeline_config = {\n    'extractors': [\n        {'type': 'csp', 'params': {'n_components': 6}},\n        {'type': 'band_power', 'params': {'bands': {'mu': (8, 12), 'beta': (12, 30)}}}\n    ],\n    'mode': 'concatenate'\n}\n\npipeline_from_config = FeatureExtractionPipeline.from_config(pipeline_config)\nprint(\"\\n=== Scenario 3: Config-driven ===\")\nprint(f\"Pipeline created from config: {pipeline_from_config}\")\n\n# ============================================\n# Scenario 4: Adaptive Pipeline (Phase 4 Preview)\n# ============================================\ndef select_pipeline_by_quality(signal_quality):\n    \"\"\"\n    Select feature pipeline based on signal quality.\n    This is a preview of Phase 4 APA integration.\n    \n    Args:\n        signal_quality: dict with 'snr' key\n        \n    Returns:\n        Configured FeatureExtractionPipeline\n    \"\"\"\n    if signal_quality.get('snr', 0) > 10:\n        # High quality: use more components\n        return create_motor_imagery_pipeline(n_csp_components=8, sampling_rate=250)\n    else:\n        # Low quality: fewer parameters, more robust\n        return create_motor_imagery_pipeline(n_csp_components=4, sampling_rate=250)\n\nprint(\"\\n=== Scenario 4: Adaptive Pipeline (Phase 4 Preview) ===\")\nprint(\"Pipeline selection based on signal quality defined\")\nprint(\"- High SNR (>10): 8 CSP components\")\nprint(\"- Low SNR (\u226410): 4 CSP components\")",
      "execution_count": null,
      "metadata": {},
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## Step 6: Classification with LDA\n\nLinear Discriminant Analysis (LDA) is a classic and effective classifier for CSP features.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Step 6.1: Train LDA Classifier\nfrom src.classifiers import create_lda_classifier\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n\n# Create and train LDA on CSP features\nlda = create_lda_classifier(n_classes=4)\nlda.fit(X_train_csp, y_train)\n\n# Predict\ny_pred_lda = lda.predict(X_test_csp)\ny_prob_lda = lda.predict_proba(X_test_csp)\n\n# Evaluate\nacc_lda = accuracy_score(y_test, y_pred_lda)\n\nprint(f\"\\n=== CSP + LDA Results ===\")\nprint(f\"Accuracy: {acc_lda:.4f}\")\nprint(f\"\\nClassification Report:\")\nprint(classification_report(y_test, y_pred_lda, target_names=['Left', 'Right', 'Feet', 'Tongue']))",
      "execution_count": null,
      "metadata": {},
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Step 6.2: Confusion Matrix\nimport seaborn as sns\n\ncm = confusion_matrix(y_test, y_pred_lda)\n\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n            xticklabels=['Left', 'Right', 'Feet', 'Tongue'],\n            yticklabels=['Left', 'Right', 'Feet', 'Tongue'])\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.title(f'CSP + LDA Confusion Matrix (Accuracy: {acc_lda:.2%})')\nplt.show()",
      "execution_count": null,
      "metadata": {},
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## Step 7: Classification with SVM\n\nSupport Vector Machine with RBF kernel often achieves higher accuracy than LDA for complex decision boundaries.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Step 7.1: Train SVM Classifier\nfrom src.classifiers import create_svm_classifier\n\n# Create and train SVM with RBF kernel\nsvm = create_svm_classifier(kernel='rbf', C=1.0, gamma='scale', n_classes=4)\nsvm.fit(X_train_csp, y_train)\n\n# Predict\ny_pred_svm = svm.predict(X_test_csp)\nacc_svm = accuracy_score(y_test, y_pred_svm)\n\nprint(f\"\\n=== CSP + SVM Results ===\")\nprint(f\"Accuracy: {acc_svm:.4f}\")\nprint(f\"\\nClassification Report:\")\nprint(classification_report(y_test, y_pred_svm, target_names=['Left', 'Right', 'Feet', 'Tongue']))\nprint(f\"\\nSupport Vectors per class: {svm.n_support_}\")",
      "execution_count": null,
      "metadata": {},
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## Step 8: EEGNet Deep Learning Classifier\n\nEEGNet is a compact CNN designed specifically for EEG classification. It can learn directly from raw/preprocessed EEG without manual feature extraction.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Step 8.1: Create and Train EEGNet\nimport torch\nfrom src.classifiers import create_eegnet_classifier\n\n# Check device\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f\"Using device: {device}\")\n\n# Create EEGNet\neegnet = create_eegnet_classifier(\n    n_classes=4,\n    n_channels=22,\n    n_samples=1000,  # 4 seconds at 250 Hz\n    F1=8,\n    D=2,\n    dropout_rate=0.5,\n    learning_rate=0.001,\n    device=device\n)\n\nprint(f\"\\n=== EEGNet Model ===\")\nprint(f\"Parameters: {eegnet.count_parameters()}\")",
      "execution_count": null,
      "metadata": {},
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Step 8.2: Train EEGNet\n# Split training data for validation\nX_train_dl, X_val_dl, y_train_dl, y_val_dl = train_test_split(\n    X_train, y_train, test_size=0.15, random_state=42, stratify=y_train\n)\n\n# Train\nprint(\"\\n=== Training EEGNet ===\")\neegnet.fit(\n    X_train_dl.astype(np.float32), \n    y_train_dl,\n    validation_data=(X_val_dl.astype(np.float32), y_val_dl),\n    epochs=50,\n    batch_size=32,\n    verbose=1\n)",
      "execution_count": null,
      "metadata": {},
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Step 8.3: Evaluate EEGNet\ny_pred_eegnet = eegnet.predict(X_test.astype(np.float32))\nacc_eegnet = accuracy_score(y_test, y_pred_eegnet)\n\nprint(f\"\\n=== EEGNet Results ===\")\nprint(f\"Accuracy: {acc_eegnet:.4f}\")\nprint(f\"\\nClassification Report:\")\nprint(classification_report(y_test, y_pred_eegnet, target_names=['Left', 'Right', 'Feet', 'Tongue']))",
      "execution_count": null,
      "metadata": {},
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Step 8.4: Plot Training History\nhistory = eegnet.get_training_history()\n\nif history:\n    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n    \n    # Loss\n    ax1 = axes[0]\n    ax1.plot(history['train_loss'], label='Train Loss')\n    ax1.plot(history['val_loss'], label='Val Loss')\n    ax1.set_xlabel('Epoch')\n    ax1.set_ylabel('Loss')\n    ax1.set_title('EEGNet Training Loss')\n    ax1.legend()\n    ax1.grid(True, alpha=0.3)\n    \n    # Accuracy\n    ax2 = axes[1]\n    ax2.plot(history['train_accuracy'], label='Train Acc')\n    ax2.plot(history['val_accuracy'], label='Val Acc')\n    ax2.set_xlabel('Epoch')\n    ax2.set_ylabel('Accuracy')\n    ax2.set_title('EEGNet Training Accuracy')\n    ax2.legend()\n    ax2.grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.show()",
      "execution_count": null,
      "metadata": {},
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## Step 9: Model Comparison",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Step 9.1: Compare All Models\nfrom sklearn.metrics import cohen_kappa_score\n\nresults = {\n    'Model': ['CSP + LDA', 'CSP + SVM', 'EEGNet'],\n    'Accuracy': [acc_lda, acc_svm, acc_eegnet],\n    'Kappa': [\n        cohen_kappa_score(y_test, y_pred_lda),\n        cohen_kappa_score(y_test, y_pred_svm),\n        cohen_kappa_score(y_test, y_pred_eegnet)\n    ]\n}\n\nimport pandas as pd\nresults_df = pd.DataFrame(results)\nresults_df = results_df.sort_values('Accuracy', ascending=False)\n\nprint(\"\\n=== Model Comparison ===\")\nprint(results_df.to_string(index=False))\n\n# Visualization\nfig, ax = plt.subplots(figsize=(10, 5))\nx = np.arange(len(results['Model']))\nwidth = 0.35\n\nbars1 = ax.bar(x - width/2, results['Accuracy'], width, label='Accuracy', color='steelblue')\nbars2 = ax.bar(x + width/2, results['Kappa'], width, label='Kappa', color='darkorange')\n\nax.set_ylabel('Score')\nax.set_title('Model Comparison: Accuracy & Kappa')\nax.set_xticks(x)\nax.set_xticklabels(results['Model'])\nax.legend()\nax.axhline(y=0.85, color='green', linestyle='--', alpha=0.5, label='Target (85%)')\nax.set_ylim([0, 1])\nax.grid(True, alpha=0.3, axis='y')\n\nplt.tight_layout()\nplt.show()",
      "execution_count": null,
      "metadata": {},
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## Step 10: Cross-Subject Evaluation (LOSO)",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Step 10.1: Leave-One-Subject-Out Cross-Validation\ndef run_loso_cv(data_dir, subjects, use_csp_lda=True):\n    \"\"\"\n    Run Leave-One-Subject-Out cross-validation.\n    \n    Args:\n        data_dir: Path to dataset\n        subjects: List of subject IDs\n        use_csp_lda: If True, use CSP+LDA; else use EEGNet\n        \n    Returns:\n        Dict with results per subject and aggregate metrics\n    \"\"\"\n    loader = BCICIV2aLoader()\n    preproc = create_standard_pipeline(sampling_rate=250, notch_freq=50.0, low_freq=8.0, high_freq=30.0)\n    preproc.initialize()\n    \n    results = []\n    \n    for test_subject in subjects:\n        print(f\"\\nTest Subject: {test_subject}\")\n        \n        # Collect train and test data\n        X_train_all, y_train_all = [], []\n        X_test_sub, y_test_sub = None, None\n        \n        for subject in subjects:\n            file_path = os.path.join(data_dir, f\"{subject}T.mat\")\n            if not os.path.exists(file_path):\n                continue\n            \n            eeg_data = loader.load(file_path)\n            X, y = loader.extract_trials(eeg_data)\n            X = preproc.process(X)\n            \n            if subject == test_subject:\n                X_test_sub, y_test_sub = X, y\n            else:\n                X_train_all.append(X)\n                y_train_all.append(y)\n        \n        X_train_all = np.concatenate(X_train_all, axis=0)\n        y_train_all = np.concatenate(y_train_all, axis=0)\n        \n        # Train and evaluate\n        if use_csp_lda:\n            csp = create_csp_extractor(n_components=6, sampling_rate=250)\n            X_train_feat = csp.fit_extract(X_train_all, y_train_all)\n            X_test_feat = csp.extract(X_test_sub)\n            \n            clf = create_lda_classifier(n_classes=4)\n            clf.fit(X_train_feat, y_train_all)\n            y_pred = clf.predict(X_test_feat)\n        else:\n            clf = create_eegnet_classifier(\n                n_classes=4, n_channels=22, n_samples=1000, device='cpu'\n            )\n            clf.fit(X_train_all.astype(np.float32), y_train_all, epochs=30, verbose=0)\n            y_pred = clf.predict(X_test_sub.astype(np.float32))\n        \n        acc = accuracy_score(y_test_sub, y_pred)\n        kappa = cohen_kappa_score(y_test_sub, y_pred)\n        \n        results.append({\n            'subject': test_subject,\n            'accuracy': acc,\n            'kappa': kappa\n        })\n        print(f\"  Accuracy: {acc:.4f}, Kappa: {kappa:.4f}\")\n    \n    # Aggregate\n    accs = [r['accuracy'] for r in results]\n    kappas = [r['kappa'] for r in results]\n    \n    return {\n        'results': results,\n        'mean_accuracy': np.mean(accs),\n        'std_accuracy': np.std(accs),\n        'mean_kappa': np.mean(kappas),\n        'std_kappa': np.std(kappas)\n    }\n\nprint(\"\u2705 LOSO CV function defined\")",
      "execution_count": null,
      "metadata": {},
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Step 10.2: Run LOSO Cross-Validation\n# Uncomment to run (takes several minutes)\n# subjects = ['A01', 'A02', 'A03', 'A04', 'A05', 'A06', 'A07', 'A08', 'A09']\n# loso_results = run_loso_cv(DATA_DIR, subjects, use_csp_lda=True)\n# print(f\"\\n=== LOSO Results (CSP + LDA) ===\")\n# print(f\"Mean Accuracy: {loso_results['mean_accuracy']:.4f} \u00b1 {loso_results['std_accuracy']:.4f}\")\n# print(f\"Mean Kappa: {loso_results['mean_kappa']:.4f} \u00b1 {loso_results['std_kappa']:.4f}\")",
      "execution_count": null,
      "metadata": {},
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## Step 11: Results Export & Sharing System\n\nA comprehensive results export system for:\n1. **AI Assistant Sharing**: Copy-paste ready markdown format\n2. **Experiment Tracking**: Log and compare experiments\n3. **Drive Export**: Save to Google Drive for persistence",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Step 11.1: ResultsExporter Class\nfrom datetime import datetime\nimport json\nimport os\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import confusion_matrix, accuracy_score, cohen_kappa_score\n\nclass ResultsExporter:\n    \"\"\"\n    Export results in shareable formats for AI assistant collaboration.\n    \n    Features:\n    - Markdown format for copy-paste sharing\n    - JSON format for programmatic parsing\n    - CSV export for Drive storage\n    - Confusion matrix visualization\n    \n    Usage:\n        exporter = ResultsExporter('phase3_experiment_1')\n        exporter.log_config({'csp_components': 6, 'classifier': 'lda'})\n        exporter.log_metric('accuracy', 0.85, subset='test')\n        exporter.log_confusion_matrix(y_true, y_pred, ['Left', 'Right', 'Feet', 'Tongue'])\n        \n        # Share with AI\n        exporter.print_share_ready()\n        \n        # Save to Drive\n        exporter.save_to_drive('/content/drive/MyDrive/llm-eeg/experiments')\n    \"\"\"\n    \n    def __init__(self, experiment_name: str):\n        self.experiment_name = experiment_name\n        self.timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n        self.date_str = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n        self.config = {}\n        self.metrics = {'train': {}, 'val': {}, 'test': {}}\n        self.confusion_matrices = {}\n        self.observations = []\n        \n    def log_config(self, config: dict):\n        \"\"\"Log experiment configuration.\"\"\"\n        self.config.update(config)\n        \n    def log_metric(self, name: str, value: float, subset: str = 'test'):\n        \"\"\"Log a metric value for train/val/test subset.\"\"\"\n        if subset not in self.metrics:\n            self.metrics[subset] = {}\n        self.metrics[subset][name] = value\n        \n    def log_confusion_matrix(self, y_true, y_pred, class_names: list):\n        \"\"\"Log confusion matrix.\"\"\"\n        cm = confusion_matrix(y_true, y_pred)\n        self.confusion_matrices['test'] = {\n            'matrix': cm.tolist(),\n            'class_names': class_names\n        }\n        \n    def add_observation(self, observation: str):\n        \"\"\"Add an observation note.\"\"\"\n        self.observations.append(observation)\n        \n    def get_metric(self, name: str, subset: str = 'test'):\n        \"\"\"Get a logged metric value.\"\"\"\n        return self.metrics.get(subset, {}).get(name, None)\n    \n    def to_markdown(self) -> str:\n        \"\"\"\n        Generate markdown format for AI assistant sharing.\n        \n        Returns:\n            Markdown string ready for copy-paste\n        \"\"\"\n        md = []\n        md.append(f\"## Experiment: {self.experiment_name}\")\n        md.append(f\"**Date:** {self.date_str}\")\n        \n        # Config\n        if self.config:\n            config_str = ', '.join([f\"{k}={v}\" for k, v in self.config.items()])\n            md.append(f\"**Config:** {config_str}\")\n        \n        md.append(\"\")\n        md.append(\"### Results\")\n        \n        # Results table\n        if any(self.metrics.values()):\n            md.append(\"| Metric | Train | Val | Test |\")\n            md.append(\"|--------|-------|-----|------|\")\n            \n            all_metrics = set()\n            for subset_metrics in self.metrics.values():\n                all_metrics.update(subset_metrics.keys())\n            \n            for metric in sorted(all_metrics):\n                train_val = self.metrics['train'].get(metric, '-')\n                val_val = self.metrics['val'].get(metric, '-')\n                test_val = self.metrics['test'].get(metric, '-')\n                \n                if isinstance(train_val, float):\n                    train_val = f\"{train_val:.4f}\"\n                if isinstance(val_val, float):\n                    val_val = f\"{val_val:.4f}\"\n                if isinstance(test_val, float):\n                    test_val = f\"{test_val:.4f}\"\n                    \n                md.append(f\"| {metric} | {train_val} | {val_val} | {test_val} |\")\n        \n        # Confusion Matrix\n        if 'test' in self.confusion_matrices:\n            cm_data = self.confusion_matrices['test']\n            cm = cm_data['matrix']\n            names = cm_data['class_names']\n            \n            md.append(\"\")\n            md.append(\"### Confusion Matrix (Test)\")\n            header = \"| | \" + \" | \".join(names) + \" |\"\n            md.append(header)\n            md.append(\"|--\" + \"|---\" * len(names) + \"|\")\n            \n            for i, row in enumerate(cm):\n                row_str = f\"| **{names[i]}** | \" + \" | \".join(map(str, row)) + \" |\"\n                md.append(row_str)\n        \n        # Observations\n        if self.observations:\n            md.append(\"\")\n            md.append(\"### Observations\")\n            for obs in self.observations:\n                md.append(f\"- {obs}\")\n        \n        return \"\\n\".join(md)\n    \n    def to_json(self) -> str:\n        \"\"\"Generate JSON format for programmatic parsing.\"\"\"\n        data = {\n            'experiment_name': self.experiment_name,\n            'timestamp': self.timestamp,\n            'date': self.date_str,\n            'config': self.config,\n            'metrics': self.metrics,\n            'confusion_matrices': self.confusion_matrices,\n            'observations': self.observations\n        }\n        return json.dumps(data, indent=2)\n    \n    def to_csv(self, path: str):\n        \"\"\"Save metrics to CSV.\"\"\"\n        rows = []\n        for subset, metrics in self.metrics.items():\n            for metric, value in metrics.items():\n                rows.append({\n                    'experiment': self.experiment_name,\n                    'subset': subset,\n                    'metric': metric,\n                    'value': value\n                })\n        df = pd.DataFrame(rows)\n        df.to_csv(path, index=False)\n        print(f\"Saved to {path}\")\n    \n    def save_to_drive(self, drive_path: str):\n        \"\"\"Save all results to Google Drive.\"\"\"\n        exp_dir = os.path.join(drive_path, f\"exp_{self.timestamp}\")\n        os.makedirs(exp_dir, exist_ok=True)\n        \n        # Save JSON\n        with open(os.path.join(exp_dir, 'results.json'), 'w') as f:\n            f.write(self.to_json())\n        \n        # Save CSV\n        self.to_csv(os.path.join(exp_dir, 'metrics.csv'))\n        \n        # Save Markdown\n        with open(os.path.join(exp_dir, 'summary.md'), 'w') as f:\n            f.write(self.to_markdown())\n        \n        print(f\"\\n\u2705 Results saved to: {exp_dir}\")\n        return exp_dir\n    \n    def print_share_ready(self):\n        \"\"\"Print copy-paste ready format for AI assistant.\"\"\"\n        print(\"=\" * 60)\n        print(\"\ud83d\udccb COPY THIS TO SHARE WITH AI ASSISTANT:\")\n        print(\"=\" * 60)\n        print(self.to_markdown())\n        print(\"=\" * 60)\n\n\n# Demo usage\nprint(\"\u2705 ResultsExporter class defined\")\nprint(\"\\nExample usage:\")\nprint('  exporter = ResultsExporter(\"phase3_csp_lda\")')\nprint('  exporter.log_config({\"csp_components\": 6})')\nprint('  exporter.log_metric(\"accuracy\", 0.85)')\nprint('  exporter.print_share_ready()')",
      "execution_count": null,
      "metadata": {},
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Step 11.2: ExperimentTracker Class\nclass ExperimentTracker:\n    \"\"\"\n    Track and compare multiple experiments.\n    \n    Usage:\n        tracker = ExperimentTracker('/content/drive/MyDrive/llm-eeg/experiments')\n        exp_id = tracker.new_experiment('csp_lda_test', {'csp': 6, 'clf': 'lda'})\n        tracker.log_results(exp_id, {'accuracy': 0.85, 'kappa': 0.80})\n        tracker.compare_experiments(['exp1', 'exp2'])\n    \"\"\"\n    \n    def __init__(self, save_dir: str = '/content/drive/MyDrive/llm-eeg/experiments'):\n        self.save_dir = save_dir\n        self.experiments = {}\n        self._load_existing()\n        \n    def _load_existing(self):\n        \"\"\"Load existing experiment log if available.\"\"\"\n        log_path = os.path.join(self.save_dir, 'experiment_log.json')\n        if os.path.exists(log_path):\n            with open(log_path, 'r') as f:\n                self.experiments = json.load(f)\n    \n    def _save_log(self):\n        \"\"\"Save experiment log.\"\"\"\n        os.makedirs(self.save_dir, exist_ok=True)\n        log_path = os.path.join(self.save_dir, 'experiment_log.json')\n        with open(log_path, 'w') as f:\n            json.dump(self.experiments, f, indent=2)\n    \n    def new_experiment(self, name: str, config: dict) -> str:\n        \"\"\"\n        Start a new experiment.\n        \n        Args:\n            name: Experiment name\n            config: Configuration dict\n            \n        Returns:\n            experiment_id: Unique ID for this experiment\n        \"\"\"\n        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n        exp_id = f\"{name}_{timestamp}\"\n        \n        self.experiments[exp_id] = {\n            'name': name,\n            'config': config,\n            'timestamp': timestamp,\n            'date': datetime.now().isoformat(),\n            'results': {}\n        }\n        self._save_log()\n        return exp_id\n    \n    def log_results(self, experiment_id: str, results: dict):\n        \"\"\"Log results for an experiment.\"\"\"\n        if experiment_id in self.experiments:\n            self.experiments[experiment_id]['results'].update(results)\n            self._save_log()\n    \n    def compare_experiments(self, experiment_ids: list = None) -> pd.DataFrame:\n        \"\"\"\n        Compare multiple experiments.\n        \n        Args:\n            experiment_ids: List of experiment IDs to compare. \n                          If None, compare all.\n        \n        Returns:\n            DataFrame with comparison\n        \"\"\"\n        if experiment_ids is None:\n            experiment_ids = list(self.experiments.keys())\n        \n        rows = []\n        for exp_id in experiment_ids:\n            if exp_id in self.experiments:\n                exp = self.experiments[exp_id]\n                row = {\n                    'experiment_id': exp_id,\n                    'name': exp['name'],\n                    'date': exp.get('date', 'N/A')\n                }\n                row.update(exp.get('config', {}))\n                row.update(exp.get('results', {}))\n                rows.append(row)\n        \n        df = pd.DataFrame(rows)\n        return df\n    \n    def get_best_experiment(self, metric: str = 'accuracy') -> dict:\n        \"\"\"Find the best experiment by a metric.\"\"\"\n        best_id = None\n        best_value = -float('inf')\n        \n        for exp_id, exp in self.experiments.items():\n            value = exp.get('results', {}).get(metric, -float('inf'))\n            if value > best_value:\n                best_value = value\n                best_id = exp_id\n        \n        if best_id:\n            return {'id': best_id, 'value': best_value, **self.experiments[best_id]}\n        return None\n    \n    def export_all(self) -> str:\n        \"\"\"Export all experiments as markdown.\"\"\"\n        md = [\"# Experiment History\\n\"]\n        \n        for exp_id, exp in self.experiments.items():\n            md.append(f\"## {exp['name']}\")\n            md.append(f\"- **ID:** {exp_id}\")\n            md.append(f\"- **Date:** {exp.get('date', 'N/A')}\")\n            md.append(f\"- **Config:** {exp.get('config', {})}\")\n            md.append(f\"- **Results:** {exp.get('results', {})}\")\n            md.append(\"\")\n        \n        return \"\\n\".join(md)\n\n\nprint(\"\u2705 ExperimentTracker class defined\")",
      "execution_count": null,
      "metadata": {},
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Step 11.3: Export Current Results with ResultsExporter\nfrom sklearn.metrics import cohen_kappa_score\n\n# Create exporter for this experiment\nexporter = ResultsExporter('phase3_subject_A01')\n\n# Log configuration\nexporter.log_config({\n    'subject': 'A01',\n    'csp_components': 6,\n    'classifiers': 'LDA, SVM, EEGNet',\n    'preprocessing': 'Notch(50Hz) + Bandpass(8-30Hz) + ZScore'\n})\n\n# Log metrics for best model (CSP + LDA as example)\nexporter.log_metric('accuracy', acc_lda, 'test')\nexporter.log_metric('kappa', cohen_kappa_score(y_test, y_pred_lda), 'test')\n\n# Log confusion matrix\nexporter.log_confusion_matrix(y_test, y_pred_lda, ['Left', 'Right', 'Feet', 'Tongue'])\n\n# Add observations\nbest_model = 'CSP+LDA' if acc_lda >= max(acc_svm, acc_eegnet) else ('CSP+SVM' if acc_svm >= acc_eegnet else 'EEGNet')\nexporter.add_observation(f\"Best model: {best_model}\")\nexporter.add_observation(f\"Target (85%): {'Achieved \u2705' if max(acc_lda, acc_svm, acc_eegnet) >= 0.85 else 'Not yet \u274c'}\")\n\n# Print shareable format\nexporter.print_share_ready()",
      "execution_count": null,
      "metadata": {},
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Step 11.4: Model Comparison Helper\ndef compare_models(results_dict: dict) -> pd.DataFrame:\n    \"\"\"\n    Compare multiple models and generate shareable table.\n    \n    Args:\n        results_dict: Dict of {model_name: {'y_pred': array, 'accuracy': float, ...}}\n        \n    Returns:\n        Comparison DataFrame\n    \"\"\"\n    comparison = []\n    for name, data in results_dict.items():\n        row = {'Model': name}\n        row.update({k: v for k, v in data.items() if k not in ['y_pred']})\n        comparison.append(row)\n    \n    df = pd.DataFrame(comparison)\n    df = df.sort_values('accuracy', ascending=False)\n    \n    # Print markdown format\n    print(\"\\n\ud83d\udcca MODEL COMPARISON (Copy-paste ready):\\n\")\n    print(df.to_markdown(index=False))\n    \n    return df\n\n# Compare all models\nall_results = {\n    'CSP + LDA': {\n        'accuracy': acc_lda,\n        'kappa': cohen_kappa_score(y_test, y_pred_lda),\n        'train_time': 'Fast'\n    },\n    'CSP + SVM': {\n        'accuracy': acc_svm,\n        'kappa': cohen_kappa_score(y_test, y_pred_svm),\n        'train_time': 'Fast'\n    },\n    'EEGNet': {\n        'accuracy': acc_eegnet,\n        'kappa': cohen_kappa_score(y_test, y_pred_eegnet),\n        'train_time': 'Medium'\n    }\n}\n\ncomparison_df = compare_models(all_results)",
      "execution_count": null,
      "metadata": {},
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## Step 12: Quick Reference & Summary\n\n### Results Summary\n\nPrint a comprehensive summary to share with AI assistant for analysis.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Step 12.1: Generate Summary for AI Assistant\ndef generate_summary():\n    \"\"\"Generate shareable summary.\"\"\"\n    summary = f\"\"\"\n# Phase 3 Results Summary\n\n## Dataset\n- Subject: A01\n- Trials: {X.shape[0]} ({X_train.shape[0]} train, {X_test.shape[0]} test)\n- Channels: {X.shape[1]}\n- Samples: {X.shape[2]} (4s @ 250Hz)\n- Classes: 4 (Left Hand, Right Hand, Feet, Tongue)\n\n## Feature Extraction\n- CSP: {X_train_csp.shape[1]} features ({csp._n_components} components)\n- Band Power: 3 bands (Mu, Beta-Low, Beta-High)\n\n## Classification Results\n| Model | Accuracy | Kappa |\n|-------|----------|-------|\n| CSP + LDA | {acc_lda:.4f} | {cohen_kappa_score(y_test, y_pred_lda):.4f} |\n| CSP + SVM | {acc_svm:.4f} | {cohen_kappa_score(y_test, y_pred_svm):.4f} |\n| EEGNet | {acc_eegnet:.4f} | {cohen_kappa_score(y_test, y_pred_eegnet):.4f} |\n\n## Observations\n- Best performing model: {'CSP+LDA' if acc_lda > max(acc_svm, acc_eegnet) else 'CSP+SVM' if acc_svm > acc_eegnet else 'EEGNet'}\n- Target (85%): {'Achieved \u2705' if max(acc_lda, acc_svm, acc_eegnet) >= 0.85 else 'Not yet achieved \u274c'}\n\"\"\"\n    return summary\n\nprint(generate_summary())",
      "execution_count": null,
      "metadata": {},
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Step 12.2: Quick Reference Card\nprint(\"\"\"\n\u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\n\u2551                    QUICK REFERENCE CARD                       \u2551\n\u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563\n\u2551                                                              \u2551\n\u2551  \ud83d\udcca VIEW RESULTS:                                            \u2551\n\u2551     exporter.print_share_ready()                             \u2551\n\u2551                                                              \u2551\n\u2551  \ud83d\udccb COPY FOR AI ASSISTANT:                                   \u2551\n\u2551     print(exporter.to_markdown())                            \u2551\n\u2551                                                              \u2551\n\u2551  \ud83d\udcbe SAVE TO DRIVE:                                           \u2551\n\u2551     exporter.save_to_drive('/content/drive/MyDrive/llm-eeg') \u2551\n\u2551                                                              \u2551\n\u2551  \ud83d\udd04 COMPARE MODELS:                                          \u2551\n\u2551     compare_models({'lda': lda_res, 'eegnet': eegnet_res})   \u2551\n\u2551                                                              \u2551\n\u2551  \ud83d\udcc8 EXPERIMENT TRACKING:                                     \u2551\n\u2551     tracker = ExperimentTracker(save_dir)                    \u2551\n\u2551     tracker.compare_experiments(['exp1', 'exp2'])            \u2551\n\u2551                                                              \u2551\n\u2551  \ud83c\udfaf FEATURE EXTRACTION:                                      \u2551\n\u2551     csp = create_csp_extractor(n_components=6)               \u2551\n\u2551     X_csp = csp.fit_extract(X_train, y_train)                \u2551\n\u2551                                                              \u2551\n\u2551  \ud83e\udd16 CLASSIFICATION:                                          \u2551\n\u2551     lda = create_lda_classifier(n_classes=4)                 \u2551\n\u2551     lda.fit(X_csp, y_train)                                  \u2551\n\u2551     predictions = lda.predict(X_test_csp)                    \u2551\n\u2551                                                              \u2551\n\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\n\n\ud83d\udcc1 GOOGLE DRIVE FOLDER STRUCTURE:\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nGoogle Drive/\n\u2514\u2500\u2500 MyDrive/\n    \u2514\u2500\u2500 llm-eeg/\n        \u251c\u2500\u2500 data/\n        \u2502   \u2514\u2500\u2500 BCI_Competition_IV_2a/   # Dataset\n        \u251c\u2500\u2500 experiments/\n        \u2502   \u251c\u2500\u2500 experiment_log.json      # All experiments\n        \u2502   \u2514\u2500\u2500 exp_YYYYMMDD_HHMMSS/     # Per experiment\n        \u2502       \u251c\u2500\u2500 results.json\n        \u2502       \u251c\u2500\u2500 metrics.csv\n        \u2502       \u2514\u2500\u2500 summary.md\n        \u251c\u2500\u2500 models/                       # Saved models\n        \u2502   \u251c\u2500\u2500 csp_model.pkl\n        \u2502   \u2514\u2500\u2500 lda_model.pkl\n        \u2514\u2500\u2500 reports/\n            \u2514\u2500\u2500 comparison_report.md\n\"\"\")",
      "execution_count": null,
      "metadata": {},
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## Phase 3 Complete!\n\n### Summary\n\nYou have successfully completed Phase 3: Feature Extraction & Classification:\n\n1. **CSP Feature Extraction**: 6 components, spatial patterns visualization\n2. **Band Power Features**: Mu, Beta frequency bands\n3. **Feature Pipeline**: Modular multi-extractor pipeline\n4. **LDA Classification**: Linear discriminant analysis on CSP features\n5. **SVM Classification**: RBF kernel SVM for non-linear boundaries\n6. **EEGNet**: End-to-end deep learning classifier\n7. **Model Comparison**: Accuracy and Kappa metrics\n8. **Cross-Validation**: LOSO setup for subject-independent evaluation\n9. **Results Export**: JSON export for AI assistant sharing\n\n### Next Steps: Phase 4 - Agent System\n\n- Adaptive Preprocessing Agent (APA)\n- Decision Validation Agent (DVA)\n- Q-learning policy optimization\n- Cross-trial learning\n\n---\n\n### Quick Reference\n\n```python\n# CSP Feature Extraction\ncsp = create_csp_extractor(n_components=6, sampling_rate=250)\nX_csp = csp.fit_extract(X_train, y_train)\nX_test_csp = csp.extract(X_test)\n\n# LDA Classification\nlda = create_lda_classifier(n_classes=4)\nlda.fit(X_csp, y_train)\npredictions = lda.predict(X_test_csp)\n\n# EEGNet\neegnet = create_eegnet_classifier(n_classes=4, n_channels=22, n_samples=1000)\neegnet.fit(X_train, y_train, epochs=50)\npredictions = eegnet.predict(X_test)\n```",
      "metadata": {}
    }
  ]
}